{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing pipeline\n",
    "# (1) create feature vector for each vertex-vertex pair\n",
    "# (2) data set is entire list of vertex_i-vertex_j pairs labled 0 (not connected) or 1 (connected)\n",
    "# (3) split data into test and training data\n",
    "# (4) train classifier on test data (feature vector plus label)\n",
    "# (5) evaluate classifier on training data\n",
    "#\n",
    "# HERE we actually solve the following problem: given some features, is there an edge vertex_i-vertex_j \n",
    "# or not in the graph\n",
    "# BUT in the original sense, link prediction tries to identify future vertex_i-vertex_j edges, that do not exist yet;\n",
    "# this would require time-dependent graph data showing how the graph evolves over time; \n",
    "# and because feature values change with the graph, they would require being continuously updated \n",
    "# for training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages graphframes:graphframes:0.8.2-spark3.0-s_2.12 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/sarah/miniconda3/envs/Ml_Base/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/sarah/miniconda3/envs/Ml_Base/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/sarah/.ivy2/cache\n",
      "The jars for the packages stored in: /home/sarah/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-811998f2-5336-427e-8c54-1ea0bba71c99;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "downloading https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.0-s_2.12/graphframes-0.8.2-spark3.0-s_2.12.jar ...\n",
      "\t[SUCCESSFUL ] graphframes#graphframes;0.8.2-spark3.0-s_2.12!graphframes.jar (96ms)\n",
      ":: resolution report :: resolve 3931ms :: artifacts dl 108ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   1   |   1   |   0   ||   2   |   1   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-811998f2-5336-427e-8c54-1ea0bba71c99\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 1 already retrieved (242kB/17ms)\n",
      "23/07/08 12:53:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/08 12:53:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"linkprediction\") \\\n",
    "            .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to create this dir first\n",
    "spark.sparkContext.setCheckpointDir('/home/sarah/spark/_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: integer (nullable = true)\n",
      " |-- dst: integer (nullable = true)\n",
      "\n",
      "Row(src=236, dst=186)\n",
      "Row(src=122, dst=285)\n",
      "Row(src=24, dst=346)\n",
      "Row(src=271, dst=304)\n",
      "Row(src=176, dst=9)\n",
      "5038\n"
     ]
    }
   ],
   "source": [
    "edgesDF = spark.read.csv(\"/home/sarah/spark/Documents/facebook/0.edges\", header=False, sep=' ')\n",
    "edgesDF = edgesDF.withColumnRenamed(\"_c0\", \"src\")\n",
    "edgesDF = edgesDF.withColumnRenamed(\"_c1\", \"dst\")\n",
    "# convert from string to long\n",
    "edgesDF = edgesDF.withColumnRenamed(\"src\", \"src_old\")\n",
    "edgesDF = edgesDF.withColumn(\"src\", col(\"src_old\").cast(\"int\"))\n",
    "edgesDF = edgesDF.withColumnRenamed(\"dst\", \"dst_old\")\n",
    "edgesDF = edgesDF.withColumn(\"dst\", col(\"dst_old\").cast(\"int\"))\n",
    "edgesDF = edgesDF.selectExpr(\"src\", \"dst\")\n",
    "\n",
    "edgesDF.printSchema()\n",
    "\n",
    "rows = edgesDF.take(5)\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "print(edgesDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selects the 'src' column from edgesDF and the 'dst' column from edgesDF, combines them using the union() method, and then removes duplicate values using the distinct() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(src=148)\n",
      "Row(src=243)\n",
      "Row(src=31)\n",
      "Row(src=85)\n",
      "Row(src=137)\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      "\n",
      "Row(id=148)\n",
      "Row(id=243)\n",
      "Row(id=31)\n",
      "Row(id=85)\n",
      "Row(id=137)\n",
      "333\n",
      "highest vertex id 347\n"
     ]
    }
   ],
   "source": [
    "\n",
    "verticesDF = edgesDF.select('src').union(edgesDF.select('dst')).distinct() \n",
    "\n",
    "verticesDF = verticesDF.withColumnRenamed('src','id') # renames source to id\n",
    "verticesDF.printSchema()\n",
    "\n",
    "rows = verticesDF.take(5)\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "\n",
    "print(verticesDF.count())\n",
    "\n",
    "nbrVertices = verticesDF.agg({'id': 'max'}).collect()[0]['max(id)']\n",
    "\n",
    "print(\"highest vertex id\",nbrVertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network with 333 vertices and 5038 edges\n",
      "only 4.543282020759499 % of all potential edges exist in this network!\n"
     ]
    }
   ],
   "source": [
    "graph = GraphFrame(verticesDF, edgesDF)\n",
    "print(\"network with\",graph.vertices.count(),\"vertices and\",graph.edges.count(),\"edges\")\n",
    "print(\"only\",graph.edges.count()/(graph.vertices.count()*graph.vertices.count())*100,\"% of all potential edges exist in this network!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is intended to find the neighbors of each vertex and store them in the neighbors dictionary. The length of the neighbors dictionary would indicate the number of vertices for which neighbors were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- v: struct (nullable = false)\n",
      " |    |-- id: integer (nullable = true)\n",
      " |-- n: struct (nullable = false)\n",
      " |    |-- id: integer (nullable = true)\n",
      "\n",
      "347\n"
     ]
    }
   ],
   "source": [
    "neighbors = {}\n",
    "# Find all paths in graph from v to n\n",
    "neighbors_df = graph.find('(v)-[e]->(n)').select('v','n')\n",
    "neighbors_df.printSchema()\n",
    "\n",
    "for v in range(1, nbrVertices + 1):\n",
    "    neighbors_v_df = neighbors_df.filter(col('v')['id'] == v).select('n')\n",
    "    if neighbors_v_df == None:\n",
    "        neighbors[v] = []\n",
    "    else:\n",
    "        neighbors[v] = [n['n']['id'] for n in neighbors_v_df.collect()]\n",
    "        \n",
    "print(len(neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast variables allow you to efficiently share data across all nodes (executors) in a Spark cluster. By broadcasting the neighbors dictionary, you make it available to all tasks running on the cluster without the need to transfer the data over the network for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_bc = spark.sparkContext.broadcast(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_neighbors(node):\n",
    "    global neighbors_bc \n",
    "    if node in neighbors_bc.value:\n",
    "        return neighbors_bc.value[node]\n",
    "    return []\n",
    "\n",
    "udf_feature_neighbors = udf(feature_neighbors, ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_common_neighbors(neighbors_node1, neighbors_node2):\n",
    "    common_neighbors = [node for node in neighbors_node1 if node in neighbors_node2] \n",
    "    return len(common_neighbors)\n",
    "\n",
    "udf_feature_common_neighbors = udf(feature_common_neighbors, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: long (nullable = true)\n",
      " |-- dst: long (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create DataFrame with all (existing and non-existing) edges \n",
    "\n",
    "# existing edges are labeled as 1\n",
    "edgesDF = edgesDF.withColumn(\"label\", lit(1.0))\n",
    "#edgesDF = edgesDF.withColumnRenamed(\"label\", \"label_old\")\n",
    "#edgesDF = edgesDF.withColumn(\"label\", col(\"label_old\").cast(\"float\"))\n",
    "edgesDF = edgesDF.selectExpr(\"src\", \"dst\", \"label\")\n",
    "\n",
    "# non-existing edges are labeled as 0\n",
    "# TODO develop a pure dataframe solution\n",
    "nonEdges = []\n",
    "\n",
    "vertices = verticesDF.collect()\n",
    "for v1 in vertices:\n",
    "    for v2 in vertices:\n",
    "        nonEdges.append((v1['id'],v2['id']))\n",
    "            \n",
    "nonEdgesDF = spark.createDataFrame(nonEdges, ['src','dst'])\n",
    "nonEdgesDF = nonEdgesDF.exceptAll(edgesDF.drop('label')) # sets all edges to 0 except the ones that actually exist\n",
    "nonEdgesDF = nonEdgesDF.withColumn(\"label\", lit(0.0))\n",
    "#nonEdgesDF = nonEdgesDF.withColumnRenamed(\"label\", \"label_old\")\n",
    "#nonEdgesDF = nonEdgesDF.withColumn(\"label\", col(\"label_old\").cast(\"int\"))\n",
    "nonEdgesDF = nonEdgesDF.selectExpr(\"src\", \"dst\", \"label\")\n",
    "                        \n",
    "edgesDF = edgesDF.union(nonEdgesDF)\n",
    "edgesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: long (nullable = true)\n",
      " |-- dst: long (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- src_neighbors: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- dst_neighbors: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- feature_common_neighbors: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2472:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(src=236, dst=186, label=1.0, src_neighbors=[30, 84, 248, 318, 200, 133, 25, 26, 1, 67, 252, 314, 141, 315, 322, 21, 213, 276, 224, 122, 304, 272, 303, 13, 257, 121, 280, 69, 271, 88, 62, 105, 297, 169, 142, 186], dst_neighbors=[25, 88, 271, 331, 122, 223, 113, 272, 98, 325, 303, 252, 123, 26, 56, 277, 222, 213, 142, 104, 323, 188, 322, 239, 221, 128, 236, 21, 170, 200, 67, 285, 62, 45, 203, 59, 9, 345, 178, 199, 55, 109, 341], feature_common_neighbors=15, features=DenseVector([15.0])), Row(src=122, dst=285, label=1.0, src_neighbors=[251, 136, 239, 281, 67, 232, 109, 119, 213, 276, 45, 98, 66, 9, 322, 315, 104, 62, 156, 297, 113, 345, 176, 5, 277, 26, 186, 169, 21, 271, 304, 332, 325, 188, 236, 235, 224, 252, 344, 161, 342, 170, 272, 3, 56, 303, 55, 25, 261, 200, 203, 31, 123, 280, 142, 128, 284, 274, 141, 248, 323, 285], dst_neighbors=[188, 332, 322, 203, 252, 239, 26, 25, 113, 272, 67, 232, 345, 142, 98, 10, 109, 261, 342, 56, 211, 122, 119, 60, 186, 9, 304, 221, 196, 185, 303, 277, 117, 164, 313, 315, 208, 323, 170, 213, 271, 200, 136, 246, 146, 103], feature_common_neighbors=32, features=DenseVector([32.0])), Row(src=24, dst=346, label=1.0, src_neighbors=[249, 57, 80, 92, 242, 94, 299, 101, 194, 180, 266, 187, 53, 302, 346], dst_neighbors=[187, 266, 94, 299, 254, 330, 180, 57, 80, 242, 302, 249, 24, 92, 53, 130, 300, 272, 127, 101, 196, 320, 204, 194, 184, 1], feature_common_neighbors=14, features=DenseVector([14.0])), Row(src=271, dst=304, label=1.0, src_neighbors=[56, 314, 63, 104, 298, 118, 67, 98, 200, 85, 186, 161, 313, 72, 122, 280, 268, 212, 238, 109, 82, 342, 48, 323, 318, 223, 103, 291, 169, 148, 224, 290, 26, 13, 113, 315, 9, 265, 325, 272, 128, 59, 297, 211, 261, 239, 285, 40, 176, 199, 185, 236, 170, 119, 331, 142, 141, 277, 322, 276, 25, 332, 172, 21, 79, 133, 203, 232, 30, 188, 252, 304], dst_neighbors=[13, 31, 246, 285, 315, 341, 118, 236, 136, 188, 84, 69, 141, 347, 199, 40, 200, 134, 7, 277, 211, 252, 66, 9, 168, 332, 113, 26, 280, 271, 239, 67, 56, 122, 75, 272, 334, 98, 257, 109, 203, 224, 212, 161, 170, 172, 308, 324, 21, 322, 158, 325, 249, 30], feature_common_neighbors=35, features=DenseVector([35.0])), Row(src=176, dst=9, label=1.0, src_neighbors=[188, 122, 119, 148, 85, 271, 25, 26, 170, 239, 290, 128, 9], dst_neighbors=[188, 231, 3, 271, 69, 341, 25, 232, 295, 122, 291, 285, 21, 200, 185, 133, 297, 161, 272, 30, 75, 141, 186, 315, 169, 329, 148, 56, 224, 277, 142, 134, 72, 26, 79, 113, 342, 170, 66, 156, 119, 280, 199, 85, 334, 128, 105, 67, 304, 258, 176, 276, 323, 322, 203, 252], feature_common_neighbors=10, features=DenseVector([10.0]))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# add feature vector for each edge\n",
    "# feature 1: common neighbors\n",
    "\n",
    "edgesDF = edgesDF.withColumn(\"src_neighbors\", udf_feature_neighbors(\"src\"))\n",
    "edgesDF = edgesDF.withColumn(\"dst_neighbors\", udf_feature_neighbors(\"dst\"))\n",
    "edgesDF = edgesDF.withColumn(\"feature_common_neighbors\", udf_feature_common_neighbors(\"src_neighbors\", \"dst_neighbors\"))\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in edgesDF.columns if x.startswith(\"feature\")],\n",
    "    outputCol='features')\n",
    "\n",
    "edgesDF = assembler.transform(edgesDF)\n",
    "\n",
    "edgesDF.printSchema()\n",
    "print(edgesDF.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99712, 11177)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split in train and test data\n",
    "# TODO should be stratified\n",
    "edgesDF = edgesDF.select(\"features\",\"label\")\n",
    "edgesTrainDF, edgesTestDF = edgesDF.randomSplit([0.90, 0.10])\n",
    "edgesTrainDF.count(), edgesTestDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([0.0]), label=1.0),\n",
       " Row(features=DenseVector([0.0]), label=1.0),\n",
       " Row(features=DenseVector([0.0]), label=1.0),\n",
       " Row(features=DenseVector([0.0]), label=1.0),\n",
       " Row(features=DenseVector([0.0]), label=1.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgesTrainDF.printSchema()\n",
    "edgesTrainDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([0.0]), label=1.0),\n",
       " Row(features=DenseVector([0.0]), label=1.0),\n",
       " Row(features=DenseVector([0.0]), label=1.0),\n",
       " Row(features=DenseVector([0.0]), label=1.0),\n",
       " Row(features=DenseVector([0.0]), label=1.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgesTestDF.printSchema()\n",
    "edgesTestDF.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(featuresCol = \"features\", labelCol = \"label\")\n",
    "random_forest_model = random_forest.fit(edgesTrainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_random_forest_predictions_df = random_forest_model.transform(edgesTestDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall accuracy 0.9640307523338825\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol='prediction', \n",
    "    metricName='accuracy')\n",
    "result = evaluator.evaluate(test_random_forest_predictions_df)\n",
    "print(\"overall accuracy\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([0.0]), label=1.0, rawPrediction=DenseVector([19.6964, 0.3036]), probability=DenseVector([0.9848, 0.0152]), prediction=0.0),\n",
       " Row(features=DenseVector([0.0]), label=1.0, rawPrediction=DenseVector([19.6964, 0.3036]), probability=DenseVector([0.9848, 0.0152]), prediction=0.0),\n",
       " Row(features=DenseVector([0.0]), label=1.0, rawPrediction=DenseVector([19.6964, 0.3036]), probability=DenseVector([0.9848, 0.0152]), prediction=0.0),\n",
       " Row(features=DenseVector([0.0]), label=1.0, rawPrediction=DenseVector([19.6964, 0.3036]), probability=DenseVector([0.9848, 0.0152]), prediction=0.0),\n",
       " Row(features=DenseVector([0.0]), label=1.0, rawPrediction=DenseVector([19.6964, 0.3036]), probability=DenseVector([0.9848, 0.0152]), prediction=0.0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_random_forest_predictions_df.printSchema()\n",
    "test_random_forest_predictions_df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(label=1.0, prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "labels_and_predictions_rdd = test_random_forest_predictions_df.selectExpr(\"label\", \"prediction\").rdd\n",
    "print(labels_and_predictions_rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0 precision = 0.9921418303785338\n",
      "class 0 recall = 0.9708364591147787\n",
      "class 1 precision = 0.3665987780040733\n",
      "class 1 recall = 0.6870229007633588\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "metrics = MulticlassMetrics(labels_and_predictions_rdd)\n",
    "for label in [0, 1]:\n",
    "    print(\"class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "    print(\"class %s recall = %s\" % (label, metrics.recall(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO class based evaluation\n",
    "# TODO compare with baseline / random predict\n",
    "# TODO try different classifier algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
